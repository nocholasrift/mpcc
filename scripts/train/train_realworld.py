from logging import exception
import os
import sys
import json
import genpy
import torch
import yaml
import random
import sqlite3
import torch.nn as nn
import torch.optim as optim
import rlkit.torch.pytorch_util as ptu
import numpy as np
import gymnasium as gym
import rospkg
import roslib.message
import rlkit.torch.pytorch_util as ptu

from mpcc.msg import RLState

from torch.utils.tensorboard import SummaryWriter

from tqdm import tqdm
from rlkit.torch.sac.sac import SACTrainer
from rlkit.torch.networks import ConcatMlp
from rlkit.torch.sac.policies import TanhGaussianPolicy


class CustomEnv(gym.Env):
    def __init__(self):
        super(CustomEnv, self).__init__()

        self.state_dim = 11
        self.action_dim = 2

    def set_obs_space(self):

        low = np.array(
            np.zeros(self.state_dim),
            dtype=np.float64,
        )
        high = np.array(
            np.ones(self.state_dim),
            dtype=np.float64,
        )

        self.observation_space = gym.spaces.Box(low=low, high=high, dtype=np.float64)

        self.state = np.zeros(self.state_dim, dtype=np.float64)

    def set_action_space(self):
        # define action space
        self.action_space = gym.spaces.Box(
            low=np.array(-1.0),
            high=np.array(1.0),
            dtype=np.float64,
        )

        ptu.set_gpu_mode(True)


class TrainManager:
    def __init__(self, buffer_fname, log_fname, batch_size, env):

        # self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.writer = SummaryWriter(log_fname)
        self.global_step = 0

        rospack = rospkg.RosPack()
        base_path = rospack.get_path("mpcc")
        param_file = os.path.join(base_path, "params", "train.yaml")

        with open(param_file, "r") as f:
            yaml_data = yaml.safe_load(f)

        self.batch_size = batch_size
        self.buffer_fname = buffer_fname

        self.obs_dim = env.state_dim
        self.action_dim = env.action_dim
        self.hidden_dim = yaml_data["hidden_dims"]

        self.buffer_size = yaml_data["buffer_size"]

        # Define networks
        self.qf1 = ConcatMlp(
            input_size=self.obs_dim + self.action_dim,
            output_size=1,
            hidden_sizes=[self.hidden_dim, self.hidden_dim],
        ).to(ptu.device)
        self.qf2 = ConcatMlp(
            input_size=self.obs_dim + self.action_dim,
            output_size=1,
            hidden_sizes=[self.hidden_dim, self.hidden_dim],
        ).to(ptu.device)
        self.target_qf1 = ConcatMlp(
            input_size=self.obs_dim + self.action_dim,
            output_size=1,
            hidden_sizes=[self.hidden_dim, self.hidden_dim],
        ).to(ptu.device)
        self.target_qf2 = ConcatMlp(
            input_size=self.obs_dim + self.action_dim,
            output_size=1,
            hidden_sizes=[self.hidden_dim, self.hidden_dim],
        ).to(ptu.device)
        self.policy = TanhGaussianPolicy(
            obs_dim=self.obs_dim,
            action_dim=self.action_dim,
            hidden_sizes=[self.hidden_dim, self.hidden_dim],
        ).to(ptu.device)

        self.env = env

        # Define SAC trainer
        self.trainer = SACTrainer(
            env=self.env,
            policy=self.policy,
            qf1=self.qf1,
            qf2=self.qf2,
            target_qf1=self.target_qf1,
            target_qf2=self.target_qf2,
            discount=0.99,
            reward_scale=1.0,
            soft_target_tau=5e-3,
            target_update_period=1,
            policy_lr=1e-5,
            qf_lr=1e-5,
            use_automatic_entropy_tuning=True,
        )

        # try to connect to the database
        self.conn = None
        try:
            self.conn = sqlite3.connect(buffer_fname)
        except Exception as e:
            print("Error connecting to database: ", str(e))
            sys.exit(-1)

    def deserialize_state(self, serialized_str, msg_type):
        try:
            serialized_bytes = bytes.fromhex(serialized_str)
        except ValueError as e:
            print("Error deserializing state: ", str(e))
            print("serialized_str: ", serialized_str)
            return None

        new_msg = msg_type()
        new_msg.deserialize(serialized_bytes)

        ret = []
        for elem in new_msg.state:
            ret.append(elem)

        ret.append(1 if new_msg.solver_status else 0)

        ret = np.array(ret, dtype=np.float64)

        return ret

    def load_from_db(self):

        cur = self.conn.cursor()
        random_sample = None
        try:

            query_limit_recent = f"""
            SELECT * FROM replay_buffer
            WHERE id > (SELECT MAX(id) - {self.buffer_size} FROM replay_buffer)
            ORDER BY id DESC
            """
            cur.execute(query_limit_recent)
            recent_entries = cur.fetchall()

            # Randomly sample from the limited data set
            if len(recent_entries) < self.batch_size:
                random_sample = recent_entries
            else:
                random_sample = random.sample(recent_entries, self.batch_size)

        except Exception as e:
            print("Failed to get entries from replay buffer: ", str(e))
            return [], [], [], [], []

        labels = [description[0] for description in cur.description]

        # make mapping of label to index
        label_to_index = {label: i for i, label in enumerate(labels)}
        # rows = cur.fetchall()
        rows = random_sample

        # return states, actions, rewards, next_states, dones
        states = []
        actions = []
        rewards = []
        next_states = []
        dones = []

        for row in rows:
            prev_state = self.deserialize_state(
                row[label_to_index["prev_state"]], RLState
            )
            next_state = self.deserialize_state(
                row[label_to_index["next_state"]], RLState
            )
            action = np.array(
                row[label_to_index["action"]].split(","), dtype=np.float64
            )
            action = np.array([float(a) for a in action], dtype=np.float64)
            reward = np.array(float(row[label_to_index["reward"]]), dtype=np.float64)
            done = np.array(bool(row[label_to_index["is_done"]]), dtype=np.float64)

            states.append(prev_state)
            actions.append(action)
            rewards.append(reward)
            next_states.append(next_state)
            dones.append(done)

        states = np.array(states)
        actions = np.array(actions)
        rewards = np.array(rewards)
        next_states = np.array(next_states)
        dones = np.array(dones)
        return states, actions, rewards, next_states, dones

    def train_sac(self):
        states, actions, rewards, next_states, dones = self.load_from_db()

        # iterate over batch and check if negative rewards show up for any of the samples
        # for ind, (reward, action, state, next_state) in enumerate(
        #    zip(rewards, actions, states, next_states)
        # ):
        #    alpha_dot_blw = action[0]
        #    alpha_blw = state[10]
        #    next_alpha_blw = next_state[10]

        #    if alpha_blw < 0.05 and alpha_dot_blw < 0:
        #        print(
        #            f"reward for {alpha_dot_blw} is {reward}: {alpha_blw} -> {next_alpha_blw}"
        #        )

        states = torch.FloatTensor(states).to(ptu.device)
        # actions = torch.FloatTensor(actions).unsqueeze(1).to(ptu.device)
        actions = torch.FloatTensor(actions).to(ptu.device)
        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(ptu.device)
        next_states = torch.FloatTensor(next_states).to(ptu.device)
        dones = torch.FloatTensor(dones).unsqueeze(1).to(ptu.device)

        self.trainer.train_from_torch(
            batch={
                "observations": states,
                "actions": actions,
                "rewards": rewards,
                "next_observations": next_states,
                "terminals": dones,
            }
        )

        # Logging metrics
        eval_stats = self.trainer.get_diagnostics()

        # average reward for the episode
        episode_reward = rewards.mean().item()
        q_values_1 = self.trainer.qf1(states, actions).detach().cpu().numpy()
        q_values_2 = self.trainer.qf2(states, actions).detach().cpu().numpy()
        average_q_value_1 = q_values_1.mean()
        average_q_value_2 = q_values_2.mean()
        min_average_q_value = min(average_q_value_1, average_q_value_2)

        self.writer.add_scalar("Total Episode Reward", episode_reward, self.global_step)
        self.writer.add_scalar(
            "Average Q-Value QF1", average_q_value_1, self.global_step
        )
        self.writer.add_scalar(
            "Average Q-Value QF2", average_q_value_2, self.global_step
        )
        self.writer.add_scalar(
            "Min Average Q-Value", min_average_q_value, self.global_step
        )

        if "Policy Loss" in eval_stats:
            self.writer.add_scalar(
                "Policy Loss", eval_stats["Policy Loss"], self.global_step
            )
        if "QF1 Loss" in eval_stats:
            self.writer.add_scalar(
                "Q-Function Loss QF1", eval_stats["QF1 Loss"], self.global_step
            )
        if "QF2 Loss" in eval_stats:
            self.writer.add_scalar(
                "Q-Function Loss QF2", eval_stats["QF2 Loss"], self.global_step
            )
        if "Log Pis" in eval_stats:
            self.writer.add_scalar("Entropy", eval_stats["Log Pis"], self.global_step)
        if "Alpha" in eval_stats:
            self.writer.add_scalar("Alpha", eval_stats["Alpha"], self.global_step)
        if "Alpha Loss" in eval_stats:
            self.writer.add_scalar(
                "Alpha Loss", eval_stats["Alpha Loss"], self.global_step
            )

        self.global_step += 1

    def close(self):
        self.writer.close()
